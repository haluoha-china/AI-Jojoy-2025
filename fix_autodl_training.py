#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
autodlç¯å¢ƒè®­ç»ƒé…ç½®ä¿®å¤è„šæœ¬
ä¸“é—¨é’ˆå¯¹autodlä¸Šçš„ä¼ä¸šçŸ¥è¯†åº“é¡¹ç›®
"""

import json
import os
import random
from pathlib import Path

def check_autodl_environment():
    """æ£€æŸ¥autodlç¯å¢ƒçŠ¶æ€"""
    print("=== autodlç¯å¢ƒæ£€æŸ¥ ===")
    
    # æ£€æŸ¥å½“å‰ç›®å½•
    current_dir = os.getcwd()
    print(f"å½“å‰ç›®å½•: {current_dir}")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„LLaMA-Factoryç›®å½•
    if "LLaMA-Factory" not in current_dir:
        print("âš ï¸  è­¦å‘Š: å½“å‰ä¸åœ¨LLaMA-Factoryç›®å½•ä¸­")
        print("è¯·ç¡®ä¿åœ¨ /root/autodl-tmp/enterprise_kb/LLaMA-Factory ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    key_files = [
        "src/train.py",
        "data/comprehensive_eval.jsonl"
    ]
    
    missing_files = []
    for file_path in key_files:
        if not os.path.exists(file_path):
            missing_files.append(file_path)
    
    if missing_files:
        print(f"âŒ ç¼ºå°‘å…³é”®æ–‡ä»¶: {missing_files}")
        return False
    
    print("âœ… autodlç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    return True

def analyze_comprehensive_eval():
    """åˆ†æcomprehensive_eval.jsonlæ–‡ä»¶"""
    print("\n=== åˆ†æç°æœ‰æ•°æ®æ–‡ä»¶ ===")
    
    eval_file = "data/comprehensive_eval.jsonl"
    
    try:
        with open(eval_file, 'r', encoding='utf-8') as f:
            data = [json.loads(line.strip()) for line in f if line.strip()]
        
        print(f"æ•°æ®æ–‡ä»¶: {eval_file}")
        print(f"æ€»æ•°æ®é‡: {len(data)} æ¡")
        
        # åˆ†ææ•°æ®ç»“æ„
        if data:
            sample = data[0]
            print(f"æ•°æ®å­—æ®µ: {list(sample.keys())}")
            
            # ç»Ÿè®¡ç¼©ç•¥è¯­æ•°é‡
            abbrs = set(item.get('abbr', '') for item in data if item.get('abbr'))
            print(f"è¦†ç›–ç¼©ç•¥è¯­æ•°é‡: {len(abbrs)} ä¸ª")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªç¼©ç•¥è¯­
            print(f"ç¤ºä¾‹ç¼©ç•¥è¯­: {list(abbrs)[:10]}")
        
        return data
        
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
        return None

def create_training_files(data):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ–‡ä»¶"""
    print("\n=== åˆ›å»ºè®­ç»ƒæ•°æ®æ–‡ä»¶ ===")
    
    if not data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥å¤„ç†")
        return False
    
    # æŒ‰ç¼©ç•¥è¯­åˆ†ç»„
    abbr_groups = {}
    for item in data:
        abbr = item.get('abbr', '')
        if abbr not in abbr_groups:
            abbr_groups[abbr] = []
        abbr_groups[abbr].append(item)
    
    print(f"æŒ‰ç¼©ç•¥è¯­åˆ†ç»„åï¼Œå…± {len(abbr_groups)} ä¸ªç¼©ç•¥è¯­")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
    train_data = []
    eval_data = []
    
    for abbr, items in abbr_groups.items():
        # éšæœºæ‰“ä¹±è¯¥ç¼©ç•¥è¯­çš„æ‰€æœ‰æ ·æœ¬
        random.shuffle(items)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        split_point = max(1, int(len(items) * 0.8))  # ç¡®ä¿æ¯ä¸ªç¼©ç•¥è¯­è‡³å°‘æœ‰ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        
        # æ·»åŠ åˆ°è®­ç»ƒé›†
        train_data.extend(items[:split_point])
        # æ·»åŠ åˆ°éªŒè¯é›†
        eval_data.extend(items[split_point:])
    
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_data)}")
    
    # ä¿å­˜è®­ç»ƒé›†
    train_file = "train_data_final.jsonl"
    with open(train_file, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # ä¿å­˜éªŒè¯é›†
    eval_file = "eval_data_final.jsonl"
    with open(eval_file, 'w', encoding='utf-8') as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_file}")
    print(f"âœ… éªŒè¯é›†å·²ä¿å­˜åˆ°: {eval_file}")
    
    return True

def create_dataset_info():
    """åˆ›å»ºdataset_info.jsoné…ç½®æ–‡ä»¶"""
    print("\n=== åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶ ===")
    
    config = {
        "company_abbreviations_train": {
            "file_name": "train_data_final.jsonl",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        },
        "company_abbreviations_eval": {
            "file_name": "eval_data_final.jsonl",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        }
    }
    
    # ç¡®ä¿dataç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_file = "data/dataset_info.json"
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    
    # æ˜¾ç¤ºé…ç½®å†…å®¹
    print("\né…ç½®æ–‡ä»¶å†…å®¹:")
    print(json.dumps(config, ensure_ascii=False, indent=2))

def generate_training_command():
    """ç”Ÿæˆæ­£ç¡®çš„è®­ç»ƒå‘½ä»¤"""
    print("\n=== ç”Ÿæˆè®­ç»ƒå‘½ä»¤ ===")
    
    command = """python src/train.py \\
  --model_name_or_path /root/autodl-tmp/enterprise_kb/models/transformers/DeepSeek-R1-Distill-Qwen-7B \\
  --dataset company_abbreviations_train \\
  --eval_dataset company_abbreviations_eval \\
  --template qwen \\
  --finetuning_type lora \\
  --lora_rank 8 \\
  --lora_alpha 16 \\
  --lora_dropout 0.1 \\
  --lora_target q_proj,v_proj \\
  --output_dir ./lora_ckpt_prod \\
  --num_train_epochs 2 \\
  --per_device_train_batch_size 1 \\
  --gradient_accumulation_steps 16 \\
  --learning_rate 5e-5 \\
  --cutoff_len 256 \\
  --save_strategy steps \\
  --save_steps 100 \\
  --logging_steps 1 \\
  --overwrite_output_dir \\
  --bf16 true \\
  --metric_for_best_model eval_loss \\
  --load_best_model_at_end true"""
    
    print("æ­£ç¡®çš„è®­ç»ƒå‘½ä»¤:")
    print("=" * 80)
    print(command)
    print("=" * 80)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open("correct_training_command.sh", "w", encoding="utf-8") as f:
        f.write("#!/bin/bash\n")
        f.write("# æ­£ç¡®çš„è®­ç»ƒå‘½ä»¤ - autodlç¯å¢ƒ\n")
        f.write(command + "\n")
    
    print("\nâœ… è®­ç»ƒå‘½ä»¤å·²ä¿å­˜åˆ°: correct_training_command.sh")
    print("   å¯ä»¥ç›´æ¥è¿è¡Œ: bash correct_training_command.sh")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ autodlç¯å¢ƒè®­ç»ƒé…ç½®ä¿®å¤è„šæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_autodl_environment():
        print("\nâŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œ")
        return
    
    try:
        # åˆ†æç°æœ‰æ•°æ®
        data = analyze_comprehensive_eval()
        if not data:
            return
        
        # åˆ›å»ºè®­ç»ƒæ–‡ä»¶
        if not create_training_files(data):
            return
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        create_dataset_info()
        
        # ç”Ÿæˆè®­ç»ƒå‘½ä»¤
        generate_training_command()
        
        print("\nğŸ‰ ä¿®å¤å®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. æ¸…ç†è¾“å‡ºç›®å½•: rm -rf ./lora_ckpt_prod")
        print("2. è¿è¡Œè®­ç»ƒå‘½ä»¤: bash correct_training_command.sh")
        print("3. æˆ–è€…ç›´æ¥å¤åˆ¶ä¸Šé¢çš„è®­ç»ƒå‘½ä»¤åˆ°ç»ˆç«¯æ‰§è¡Œ")
        
    except Exception as e:
        print(f"\nâŒ ä¿®å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
