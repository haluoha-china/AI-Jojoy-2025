# ğŸš€ è®­ç»ƒé…ç½®ä¿®å¤æŒ‡å—

## ğŸ” é—®é¢˜è¯Šæ–­

æ ¹æ®kimiçš„å»ºè®®å’Œä»£ç æ£€æŸ¥ï¼Œä½ çš„è®­ç»ƒé…ç½®å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

### âŒ **ä¸»è¦é—®é¢˜**
1. **æ•°æ®é›†é…ç½®ä¸åŒ¹é…**: è®­ç»ƒå‘½ä»¤ä¸­ä½¿ç”¨çš„æ•°æ®é›†åç§°ä¸å®é™…çš„ `dataset_info.json` é…ç½®ä¸ä¸€è‡´
2. **æ–‡ä»¶è·¯å¾„é”™è¯¯**: è®­ç»ƒè„šæœ¬æ£€æŸ¥çš„æ˜¯ `train_data_final.jsonl` å’Œ `eval_data_final.jsonl`ï¼Œä½†å®é™…æ–‡ä»¶åå¯èƒ½æ˜¯ `comprehensive_eval.jsonl`
3. **ç¼ºå°‘æ­£ç¡®çš„ `dataset_info.json` é…ç½®**

## âœ… **ä¿®å¤æ­¥éª¤**

### æ­¥éª¤1: å‡†å¤‡è®­ç»ƒæ•°æ®æ–‡ä»¶

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /root/autodl-tmp/enterprise_kb/LLaMA-Factory

# è¿è¡Œæ•°æ®å‡†å¤‡è„šæœ¬
python prepare_training_files.py
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- è¯»å–ç°æœ‰çš„ `comprehensive_eval.jsonl` æ–‡ä»¶
- å°†å…¶åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
- åˆ›å»ºæ­£ç¡®çš„ `dataset_info.json` é…ç½®æ–‡ä»¶

### æ­¥éª¤2: éªŒè¯æ•°æ®æ–‡ä»¶

```bash
# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
ls -la train_data_final.jsonl eval_data_final.jsonl
ls -la data/dataset_info.json

# æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
wc -l train_data_final.jsonl eval_data_final.jsonl
```

### æ­¥éª¤3: æ¸…ç†è¾“å‡ºç›®å½•

```bash
# æ¸…ç†ä¹‹å‰çš„è®­ç»ƒè¾“å‡º
rm -rf ./lora_ckpt_prod
```

### æ­¥éª¤4: è¿è¡Œæ­£ç¡®çš„è®­ç»ƒå‘½ä»¤

```bash
python src/train.py \
  --model_name_or_path /root/autodl-tmp/enterprise_kb/models/transformers/DeepSeek-R1-Distill-Qwen-7B \
  --dataset company_abbreviations_train \
  --eval_dataset company_abbreviations_eval \
  --template qwen \
  --finetuning_type lora \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target q_proj,v_proj \
  --output_dir ./lora_ckpt_prod \
  --num_train_epochs 2 \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16 \
  --learning_rate 5e-5 \
  --cutoff_len 256 \
  --save_strategy steps \
  --save_steps 100 \
  --logging_steps 1 \
  --overwrite_output_dir \
  --bf16 true \
  --metric_for_best_model eval_loss \
  --load_best_model_at_end true
```

## ğŸ”‘ **å…³é”®é…ç½®è¯´æ˜**

### âœ… **æ­£ç¡®çš„æ•°æ®é›†åç§°**
- **`--dataset company_abbreviations_train`**: å¯¹åº” `dataset_info.json` ä¸­çš„é”®åï¼Œä¸æ˜¯æ–‡ä»¶å
- **`--eval_dataset company_abbreviations_eval`**: å¯¹åº” `dataset_info.json` ä¸­çš„é”®åï¼Œä¸æ˜¯æ–‡ä»¶å

### âœ… **æ–‡ä»¶æ˜ å°„å…³ç³»**
```json
{
  "company_abbreviations_train": {
    "file_name": "train_data_final.jsonl",  // å®é™…æ–‡ä»¶å
    "columns": { ... }
  },
  "company_abbreviations_eval": {
    "file_name": "eval_data_final.jsonl",   // å®é™…æ–‡ä»¶å
    "columns": { ... }
  }
}
```

## ğŸš¨ **å¸¸è§é”™è¯¯é¿å…**

### âŒ **é”™è¯¯ç”¨æ³•**
```bash
--dataset train_data_final.jsonl        # é”™è¯¯ï¼šè¿™æ˜¯æ–‡ä»¶å
--dataset "train_data_final.jsonl"      # é”™è¯¯ï¼šè¿™æ˜¯æ–‡ä»¶å
```

### âœ… **æ­£ç¡®ç”¨æ³•**
```bash
--dataset company_abbreviations_train    # æ­£ç¡®ï¼šè¿™æ˜¯dataset_info.jsonä¸­çš„é”®å
--eval_dataset company_abbreviations_eval # æ­£ç¡®ï¼šè¿™æ˜¯dataset_info.jsonä¸­çš„é”®å
```

## ğŸ“‹ **å¿«é€Ÿæ£€æŸ¥æ¸…å•**

- [ ] è¿è¡Œ `python prepare_training_files.py` ç”Ÿæˆè®­ç»ƒæ•°æ®
- [ ] ç¡®è®¤ `train_data_final.jsonl` å’Œ `eval_data_final.jsonl` æ–‡ä»¶å­˜åœ¨
- [ ] ç¡®è®¤ `data/dataset_info.json` é…ç½®æ–‡ä»¶å­˜åœ¨
- [ ] æ¸…ç†è¾“å‡ºç›®å½• `rm -rf ./lora_ckpt_prod`
- [ ] ä½¿ç”¨æ­£ç¡®çš„æ•°æ®é›†åç§°è¿è¡Œè®­ç»ƒå‘½ä»¤

## ğŸ¯ **é¢„æœŸç»“æœ**

ä¿®å¤å®Œæˆåï¼Œè®­ç»ƒåº”è¯¥èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨ï¼Œå¹¶ä¸”ï¼š
- è®­ç»ƒé›†å’ŒéªŒè¯é›†æ­£ç¡®åŠ è½½
- æ¨¡å‹å¼€å§‹æ­£å¸¸è®­ç»ƒ
- æ£€æŸ¥ç‚¹æ­£å¸¸ä¿å­˜
- è¯„ä¼°æŒ‡æ ‡æ­£å¸¸è®¡ç®—

## ğŸ†˜ **å¦‚æœä»æœ‰é—®é¢˜**

å¦‚æœæŒ‰ç…§ä»¥ä¸Šæ­¥éª¤ä»ç„¶æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ–‡ä»¶æƒé™æ˜¯å¦æ­£ç¡®
2. Pythonç¯å¢ƒæ˜¯å¦æ¿€æ´»
3. LLaMA-Factoryæ˜¯å¦æ­£ç¡®å®‰è£…
4. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®

è¿è¡Œä»¥ä¸‹å‘½ä»¤è·å–æ›´å¤šè¯Šæ–­ä¿¡æ¯ï¼š
```bash
python prepare_training_files.py
cat data/dataset_info.json
ls -la *.jsonl
```
