# ðŸ¤– LLaMA-Factoryå¾®è°ƒæ¡†æž¶å®‰è£…æ€»ç»“

## ðŸ“… å®‰è£…æ—¥æœŸï¼š2025å¹´8æœˆ20æ—¥
## ðŸŽ¯ ç›®æ ‡ï¼šä¸ºä¼ä¸šçŸ¥è¯†åº“æ·»åŠ 7Bæ¨¡åž‹LoRAå¾®è°ƒèƒ½åŠ›

---

## ðŸ—ï¸ å®‰è£…çŽ¯å¢ƒ

### ç³»ç»Ÿé…ç½®
| **ç»„ä»¶** | **é…ç½®** | **è¯´æ˜Ž** |
|----------|----------|----------|
| æ“ä½œç³»ç»Ÿ | Ubuntu 22.04 | åŸºç¡€ç³»ç»Ÿ |
| Pythonç‰ˆæœ¬ | 3.12.3 | condaçŽ¯å¢ƒ |
| GPU | RTX 4090D 24GB | è®­ç»ƒç¡¬ä»¶ |
| å­˜å‚¨ | ç³»ç»Ÿç›˜30GB + æ•°æ®ç›˜50GB | å­˜å‚¨ç­–ç•¥ |
| çŽ¯å¢ƒ | kb_enterprise | condaçŽ¯å¢ƒ |

### å­˜å‚¨ç­–ç•¥
- **ç³»ç»Ÿç›˜ï¼ˆ30GBï¼‰**ï¼šæ“ä½œç³»ç»Ÿã€åŸºç¡€è½¯ä»¶ã€condaçŽ¯å¢ƒ
- **æ•°æ®ç›˜ï¼ˆ50GBï¼‰**ï¼šLLaMA-Factoryã€è®­ç»ƒæ•°æ®ã€æ¨¡åž‹æ–‡ä»¶ã€æ—¥å¿—

---

## ðŸ“¦ å®‰è£…è¿‡ç¨‹

### æ­¥éª¤1ï¼šçŽ¯å¢ƒå‡†å¤‡
```bash
# æ¿€æ´»condaçŽ¯å¢ƒ
source ~/miniconda3/etc/profile.d/conda.sh
conda activate kb_enterprise

# ç¡®è®¤çŽ¯å¢ƒçŠ¶æ€
conda info --envs
python --version
```

### æ­¥éª¤2ï¼šå…‹éš†ä»“åº“
```bash
# åˆ‡æ¢åˆ°æ•°æ®ç›˜
cd /root/autodl-tmp/enterprise_kb

# å…‹éš†LLaMA-Factory
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
```

### æ­¥éª¤3ï¼šå®‰è£…ä¾èµ–
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–ï¼ˆè·³è¿‡ç‰ˆæœ¬å†²çªï¼‰
pip install \
    accelerate \
    datasets \
    peft \
    trl \
    bitsandbytes \
    scipy \
    scikit-learn \
    matplotlib \
    seaborn \
    pandas \
    numpy \
    tqdm \
    wandb \
    tensorboard \
    --index-url https://pypi.tuna.tsinghua.edu.cn/simple
```

### æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
```bash
# åˆ›å»ºè®­ç»ƒæ•°æ®ç›®å½•
mkdir -p data

# åˆ›å»ºä¼ä¸šçŸ¥è¯†åº“è®­ç»ƒæ•°æ®
cat > data/enterprise_kb.jsonl <<EOF
{"instruction":"ESSåŸ¹è®­æ¯æœˆå‡ æ¬¡ï¼Ÿ","input":"","output":"æ ¹æ®ã€ŠESSåŸ¹è®­ç®¡ç†åˆ¶åº¦ã€‹è§„å®šï¼ŒESSåŸ¹è®­æ¯æœˆè¿›è¡Œ3æ¬¡ï¼Œåˆ†åˆ«åœ¨æ¯æœˆç¬¬ä¸€å‘¨ã€ç¬¬ä¸‰å‘¨å’Œç¬¬äº”å‘¨è¿›è¡Œã€‚å…·ä½“æ—¶é—´å®‰æŽ’è¯·æŸ¥çœ‹å…¬å¸å†…ç½‘åŸ¹è®­æ—¥åŽ†ã€‚"}
{"instruction":"æ‰“å°æœºå‡ºçŽ°æ•°æ®åº“è¿žæŽ¥å¤±è´¥æ€Žä¹ˆåŠžï¼Ÿ","input":"","output":"å½“æ‰“å°æœºå‡ºçŽ°æ•°æ®åº“è¿žæŽ¥å¤±è´¥æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†ï¼š1. æ£€æŸ¥MSSQLSERVERæœåŠ¡æ˜¯å¦å¯åŠ¨ï¼›2. ç¡®è®¤1433ç«¯å£æ˜¯å¦æ”¾è¡Œï¼›3. æ£€æŸ¥ç½‘ç»œè¿žæŽ¥ï¼›4. é‡å¯æ‰“å°æœåŠ¡ã€‚è¯¦ç»†æ“ä½œæ­¥éª¤è¯·å‚è€ƒã€ŠITæ•…éšœå¤„ç†æ‰‹å†Œã€‹ç¬¬12é¡µã€‚"}
{"instruction":"ä»€ä¹ˆæ˜¯ESSç³»ç»Ÿï¼Ÿ","input":"","output":"ESSï¼ˆEmployee Self Serviceï¼‰æ˜¯å‘˜å·¥è‡ªåŠ©æœåŠ¡ç³»ç»Ÿï¼Œå‘˜å·¥å¯ä»¥é€šè¿‡è¯¥ç³»ç»Ÿè¿›è¡Œè¯·å‡ç”³è¯·ã€åŠ ç­ç”³è¯·ã€è€ƒå‹¤æŸ¥è¯¢ã€è–ªèµ„æŸ¥è¯¢ç­‰æ“ä½œã€‚ç³»ç»Ÿè®¿é—®åœ°å€ï¼šhttp://ess.company.com"}
{"instruction":"å…¬å¸å†…éƒ¨ç®€ç§°è§£é‡Š","input":"","output":"å…¬å¸å†…éƒ¨å¸¸ç”¨ç®€ç§°åŒ…æ‹¬ï¼šESSï¼ˆå‘˜å·¥è‡ªåŠ©æœåŠ¡ï¼‰ã€OAï¼ˆåŠžå…¬è‡ªåŠ¨åŒ–ï¼‰ã€ERPï¼ˆä¼ä¸šèµ„æºè§„åˆ’ï¼‰ã€CRMï¼ˆå®¢æˆ·å…³ç³»ç®¡ç†ï¼‰ã€HRï¼ˆäººåŠ›èµ„æºï¼‰ã€ITï¼ˆä¿¡æ¯æŠ€æœ¯ï¼‰ã€QAï¼ˆè´¨é‡ä¿è¯ï¼‰ã€QCï¼ˆè´¨é‡æŽ§åˆ¶ï¼‰ç­‰ã€‚"}
{"instruction":"å¦‚ä½•ç”³è¯·å¹´å‡ï¼Ÿ","input":"","output":"å¹´å‡ç”³è¯·æµç¨‹ï¼š1. ç™»å½•ESSç³»ç»Ÿï¼›2. é€‰æ‹©'è¯·å‡ç”³è¯·'ï¼›3. å¡«å†™è¯·å‡ç±»åž‹ä¸º'å¹´å‡'ï¼›4. é€‰æ‹©å¼€å§‹å’Œç»“æŸæ—¶é—´ï¼›5. å¡«å†™è¯·å‡äº‹ç”±ï¼›6. æäº¤ç”³è¯·ç­‰å¾…å®¡æ‰¹ã€‚è¯¦ç»†æ“ä½œæ­¥éª¤è¯·å‚è€ƒã€ŠESSç³»ç»Ÿä½¿ç”¨æ‰‹å†Œã€‹ã€‚"}
EOF
```

### æ­¥éª¤5ï¼šåˆ›å»ºè®­ç»ƒé…ç½®
```bash
# åˆ›å»ºé…ç½®ç›®å½•
mkdir -p configs

# åˆ›å»ºLoRAè®­ç»ƒé…ç½®æ–‡ä»¶
cat > configs/enterprise_kb_lora.yaml <<EOF
# ä¼ä¸šçŸ¥è¯†åº“LoRAå¾®è°ƒé…ç½®
model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
dataset_path: data/enterprise_kb.jsonl
template: qwen
finetuning_type: lora
output_dir: ./lora_ckpt
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 3
quantization_bit: 4
learning_rate: 3e-4
fp16: true
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1
EOF
```

---

## âœ… å®‰è£…éªŒè¯

### åŸºç¡€åŠŸèƒ½éªŒè¯
```bash
# æ£€æŸ¥LLaMA-Factoryå®‰è£…
cd /root/autodl-tmp/enterprise_kb/LLaMA-Factory
python -c "import src; print('âœ… LLaMA-FactoryåŸºç¡€åŠŸèƒ½æ­£å¸¸')"

# æ£€æŸ¥æ ¸å¿ƒåŒ…ç‰ˆæœ¬
pip list | grep -E "(accelerate|peft|trl|bitsandbytes)"
```

### éªŒè¯ç»“æžœ
| **ç»„ä»¶** | **ç‰ˆæœ¬** | **çŠ¶æ€** |
|----------|----------|----------|
| LLaMA-Factory | æœ€æ–°ç‰ˆ | âœ… å®‰è£…æˆåŠŸ |
| accelerate | 1.0.1 | âœ… å®‰è£…æˆåŠŸ |
| peft | 0.13.2 | âœ… å®‰è£…æˆåŠŸ |
| trl | 0.11.4 | âœ… å®‰è£…æˆåŠŸ |
| bitsandbytes | 0.45.5 | âœ… å®‰è£…æˆåŠŸ |
| åŸºç¡€åŠŸèƒ½ | - | âœ… æµ‹è¯•é€šè¿‡ |

---

## ðŸ“ ç›®å½•ç»“æž„

```
/root/autodl-tmp/enterprise_kb/
â”œâ”€â”€ LLaMA-Factory/                    # å¾®è°ƒæ¡†æž¶ä¸»ç›®å½•
â”‚   â”œâ”€â”€ data/                         # è®­ç»ƒæ•°æ®
â”‚   â”‚   â””â”€â”€ enterprise_kb.jsonl      # ä¼ä¸šçŸ¥è¯†åº“Q&Aæ•°æ®
â”‚   â”œâ”€â”€ configs/                      # è®­ç»ƒé…ç½®
â”‚   â”‚   â””â”€â”€ enterprise_kb_lora.yaml  # LoRAè®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â””â”€â”€ requirements.txt              # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ conda/                            # condaçŽ¯å¢ƒ
â”œâ”€â”€ models/                           # æ¨¡åž‹å­˜å‚¨ï¼ˆå¾…ä¸‹è½½ï¼‰
â”œâ”€â”€ vector_db/                        # å‘é‡æ•°æ®åº“
â””â”€â”€ logs/                             # æ—¥å¿—æ–‡ä»¶
```

---

## ðŸ”§ é…ç½®è¯´æ˜Ž

### LoRAè®­ç»ƒå‚æ•°
| **å‚æ•°** | **å€¼** | **è¯´æ˜Ž** |
|----------|--------|----------|
| model_name_or_path | deepseek-ai/DeepSeek-R1-Distill-Qwen-7B | åŸºç¡€æ¨¡åž‹ |
| finetuning_type | lora | å¾®è°ƒæ–¹å¼ |
| per_device_train_batch_size | 4 | æ‰¹æ¬¡å¤§å° |
| gradient_accumulation_steps | 4 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| num_train_epochs | 3 | è®­ç»ƒè½®æ•° |
| quantization_bit | 4 | é‡åŒ–ä½æ•° |
| learning_rate | 3e-4 | å­¦ä¹ çŽ‡ |
| lora_rank | 8 | LoRAç§© |
| lora_alpha | 16 | LoRAç¼©æ”¾å› å­ |

### è®­ç»ƒæ•°æ®æ ¼å¼
- **æ ¼å¼**ï¼šJSONLï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
- **ç»“æž„**ï¼šinstruction + input + output
- **æ•°é‡**ï¼š5ä¸ªä¼ä¸šçŸ¥è¯†åº“æ ·æœ¬
- **å†…å®¹**ï¼šESSç³»ç»Ÿã€åŸ¹è®­åˆ¶åº¦ã€æ•…éšœå¤„ç†ã€ç®€ç§°è§£é‡Šã€å¹´å‡ç”³è¯·

---

## ðŸš€ ä¸‹ä¸€æ­¥æ“ä½œ

### 1. ä¸‹è½½7BåŸºç¡€æ¨¡åž‹
```bash
# åˆ›å»ºæ¨¡åž‹ç›®å½•
mkdir -p /root/autodl-tmp/enterprise_kb/models/transformers

# ä¸‹è½½æ¨¡åž‹ï¼ˆéœ€è¦Git LFSï¼‰
cd /root/autodl-tmp/enterprise_kb/models/transformers
git lfs install
git clone https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
```

### 2. å¼€å§‹LoRAè®­ç»ƒ
```bash
cd /root/autodl-tmp/enterprise_kb/LLaMA-Factory

# å¼€å§‹è®­ç»ƒ
python src/train_bash.py --config configs/enterprise_kb_lora.yaml
```

### 3. é›†æˆåˆ°åƒé—®Agent
- é…ç½®Function Call
- é›†æˆå¾®è°ƒåŽçš„æ¨¡åž‹
- æµ‹è¯•ä¼ä¸šçŸ¥è¯†åº“é—®ç­”

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### ç‰ˆæœ¬å…¼å®¹æ€§
- **transformersç‰ˆæœ¬å†²çª**ï¼šLLaMA-Factoryè¦æ±‚4.49.0-4.55.0ï¼Œä½†çŽ¯å¢ƒä¸­æœ‰4.46.3
- **è§£å†³æ–¹æ¡ˆ**ï¼šè·³è¿‡requirements.txtï¼Œæ‰‹åŠ¨å®‰è£…æ ¸å¿ƒä¾èµ–
- **å½±å“**ï¼šåŠŸèƒ½å®Œæ•´ï¼Œä½†å¯èƒ½ç¼ºå°‘æœ€æ–°ç‰¹æ€§

### å­˜å‚¨ç®¡ç†
- **ç³»ç»Ÿç›˜ä¿æŠ¤**ï¼šé¿å…åœ¨ç³»ç»Ÿç›˜å®‰è£…å¤§åž‹æ¡†æž¶
- **æ•°æ®ç›˜ä½¿ç”¨**ï¼šæ‰€æœ‰è®­ç»ƒç›¸å…³æ–‡ä»¶å­˜å‚¨åœ¨æ•°æ®ç›˜
- **ç©ºé—´ç›‘æŽ§**ï¼šå®šæœŸæ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ

### çŽ¯å¢ƒéš”ç¦»
- **condaçŽ¯å¢ƒ**ï¼šä½¿ç”¨kb_enterpriseçŽ¯å¢ƒï¼Œé¿å…ä¾èµ–å†²çª
- **åŒ…ç®¡ç†**ï¼šä¼˜å…ˆä½¿ç”¨pipå®‰è£…ï¼Œcondaä½œä¸ºçŽ¯å¢ƒç®¡ç†
- **ç‰ˆæœ¬æŽ§åˆ¶**ï¼šè®°å½•æ‰€æœ‰åŒ…çš„ç‰ˆæœ¬å·ï¼Œä¾¿äºŽå¤çŽ°

---

## ðŸŽ¯ å®‰è£…æˆæžœ

### åŠŸèƒ½èƒ½åŠ›
- âœ… **LoRAå¾®è°ƒ**ï¼šæ”¯æŒ7Bæ¨¡åž‹å‚æ•°é«˜æ•ˆå¾®è°ƒ
- âœ… **è®­ç»ƒæ•°æ®**ï¼šä¼ä¸šçŸ¥è¯†åº“Q&Aæ ·æœ¬å‡†å¤‡å®Œæˆ
- âœ… **è®­ç»ƒé…ç½®**ï¼šå®Œæ•´çš„LoRAè®­ç»ƒå‚æ•°é…ç½®
- âœ… **çŽ¯å¢ƒéªŒè¯**ï¼šåŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡

### æŠ€æœ¯ä¼˜åŠ¿
- **å‚æ•°é«˜æ•ˆ**ï¼šLoRAæŠ€æœ¯ï¼Œè®­ç»ƒå‚æ•°é‡å°‘
- **èµ„æºå‹å¥½**ï¼š4bité‡åŒ–ï¼Œé™ä½Žæ˜¾å­˜éœ€æ±‚
- **å®šåˆ¶åŒ–**ï¼šé’ˆå¯¹ä¼ä¸šçŸ¥è¯†åº“åœºæ™¯ä¼˜åŒ–
- **å¯æ‰©å±•**ï¼šæ”¯æŒæ›´å¤šè®­ç»ƒæ•°æ®å’Œæ¨¡åž‹

---

## ðŸ“š ç›¸å…³æ–‡æ¡£

- [ç‰ˆæœ¬è®°å½•åº“](ç‰ˆæœ¬è®°å½•åº“.md)
- [å¿«é€Ÿå¯åŠ¨æ£€æŸ¥æ¸…å•](å¿«é€Ÿå¯åŠ¨æ£€æŸ¥æ¸…å•.md)
- [åŸºäºŽåƒé—®Agentçš„ä¼ä¸šçŸ¥è¯†åº“æ­å»ºæ–¹æ¡ˆ](åŸºäºŽåƒé—®Agentçš„ä¼ä¸šçŸ¥è¯†åº“æ­å»ºæ–¹æ¡ˆ.md)
- [çŽ¯å¢ƒé…ç½®ä¿¡æ¯](çŽ¯å¢ƒé…ç½®ä¿¡æ¯.txt)

---

## ðŸŽ‰ æ€»ç»“

LLaMA-Factoryå¾®è°ƒæ¡†æž¶å·²æˆåŠŸå®‰è£…å¹¶é…ç½®å®Œæˆï¼

**å½“å‰çŠ¶æ€**ï¼š
- âœ… å¾®è°ƒæ¡†æž¶ï¼šå®‰è£…æˆåŠŸï¼ŒåŠŸèƒ½æ­£å¸¸
- âœ… è®­ç»ƒæ•°æ®ï¼šä¼ä¸šçŸ¥è¯†åº“æ ·æœ¬å‡†å¤‡å®Œæˆ
- âœ… è®­ç»ƒé…ç½®ï¼šLoRAå‚æ•°é…ç½®å®Œæˆ
- ðŸ”„ ä¸‹ä¸€æ­¥ï¼šä¸‹è½½7BåŸºç¡€æ¨¡åž‹ï¼Œå¼€å§‹è®­ç»ƒ

**æŠ€æœ¯æ ˆ**ï¼š
- PyTorch 2.3.0 + CUDA 12.1 + RTX 4090
- LLaMA-Factory + accelerate + peft + trl
- FAISS-GPU + LangChain + åƒé—®Agent

çŽ°åœ¨æ‚¨å…·å¤‡äº†å®Œæ•´çš„ä¼ä¸šçŸ¥è¯†åº“LoRAå¾®è°ƒèƒ½åŠ›ï¼ðŸš€
