#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼ä¸šçŸ¥è¯†åº“å¤šæ¨¡æ€å‘é‡åŒ–ç³»ç»Ÿ - MD5å»é‡ç‰ˆæœ¬
åŸºäºæ–‡ä»¶MD5çš„æ™ºèƒ½å»é‡ï¼Œæ”¯æŒæ‰€æœ‰Officeæ–‡æ¡£æ ¼å¼
"""

import os
import sys
import logging
from pathlib import Path
import json
import pickle
from typing import List, Dict, Any, Tuple
import numpy as np
import hashlib

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¿…è¦çš„åº“
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    import easyocr
    from transformers import pipeline
    import cv2
    import librosa
    from PIL import Image
    import torch
    
    # Officeæ–‡æ¡£å¤„ç†åº“
    from docx import Document
    import openpyxl
    from pptx import Presentation
    import mammoth  # å¤„ç†.docæ–‡ä»¶
    
    print("âœ… æ‰€æœ‰ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class MD5MultimodalVectorizer:
    """åŸºäºMD5çš„å¤šæ¨¡æ€å‘é‡åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–MD5å‘é‡åŒ–å™¨"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logging.info(f"åˆå§‹åŒ–MD5å‘é‡åŒ–å™¨ï¼Œè®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
        logging.info("åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
        self.text_model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
        
        # åˆå§‹åŒ–OCRå¤„ç†å™¨
        logging.info("åˆå§‹åŒ–OCRå¤„ç†å™¨...")
        self.ocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=torch.cuda.is_available())
        
        # åˆå§‹åŒ–å›¾è¡¨ç†è§£æ¨¡å‹
        logging.info("åˆå§‹åŒ–å›¾è¡¨ç†è§£æ¨¡å‹...")
        self.image_captioner = pipeline('image-to-text', model='Salesforce/blip-image-captioning-base')
        
        # æ–‡ä»¶MD5ç¼“å­˜
        self.file_md5_cache = {}
        self.processed_files = set()
        
        logging.info("âœ… æ‰€æœ‰æ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _calculate_file_md5(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å€¼"""
        try:
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5()
                chunk = f.read(8192)
                while chunk:
                    file_hash.update(chunk)
                    chunk = f.read(8192)
                return file_hash.hexdigest()
        except Exception as e:
            logging.error(f"è®¡ç®—æ–‡ä»¶MD5å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _is_duplicate_file(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦é‡å¤ï¼ˆåŸºäºMD5ï¼‰"""
        file_md5 = self._calculate_file_md5(file_path)
        
        if not file_md5:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒMD5çš„æ–‡ä»¶
        if file_md5 in self.file_md5_cache:
            existing_file = self.file_md5_cache[file_md5]
            logging.info(f"å‘ç°é‡å¤æ–‡ä»¶: {file_path} -> {existing_file}")
            return True
        
        # è®°å½•æ–°æ–‡ä»¶çš„MD5
        self.file_md5_cache[file_md5] = file_path
        self.processed_files.add(file_path)
        return False
    
    def _fix_pil_compatibility(self, image: Image.Image, size: Tuple[int, int]) -> Image.Image:
        """ç»ˆæPILå…¼å®¹æ€§ä¿®å¤"""
        try:
            return image.resize(size, Image.Resampling.LANCZOS)
        except AttributeError:
            try:
                return image.resize(size, Image.ANTIALIAS)
            except AttributeError:
                try:
                    return image.resize(size, Image.BICUBIC)
                except AttributeError:
                    return image.resize(size)
    
    def _check_file_integrity(self, file_path: str) -> bool:
        """å¢å¼ºæ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥"""
        try:
            if not os.path.exists(file_path):
                return False
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.warning(f"æ–‡ä»¶å¤§å°ä¸º0: {file_path}")
                return False
            
            with open(file_path, 'rb') as f:
                f.read(1024)
            
            return True
            
        except Exception as e:
            logging.warning(f"æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ {file_path}: {e}")
            return False
    
    def _process_pdf_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†PDFæ–‡ä»¶"""
        try:
            from pdfminer.high_level import extract_text
            from pdfminer.layout import LAParams
            
            if not self._check_file_integrity(file_path):
                return []
            
            text = extract_text(file_path, laparams=LAParams())
            
            if not text.strip():
                logging.warning(f"PDFæ–‡ä»¶æ— æ–‡æœ¬å†…å®¹: {file_path}")
                return []
            
            chunks = self._split_text(text, max_length=1000)
            
            documents = []
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    documents.append({
                        'content': chunk.strip(),
                        'type': 'pdf_text',
                        'source': file_path,
                        'chunk_id': i,
                        'metadata': {
                            'file_type': 'pdf',
                            'file_path': file_path,
                            'file_md5': self._calculate_file_md5(file_path),
                            'chunk_size': len(chunk)
                        }
                    })
            
            logging.info(f"PDFæ–‡ä»¶å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"PDFæ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_word_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†Wordæ–‡æ¡£ (.doc, .docx)"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            logging.info(f"å¤„ç†Wordæ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨python-docxå¤„ç†.docxæ–‡ä»¶
            if file_path.endswith('.docx'):
                doc = Document(file_path)
                text_content = []
                
                # æå–æ®µè½æ–‡æœ¬
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():
                        text_content.append(paragraph.text.strip())
                
                # æå–è¡¨æ ¼æ–‡æœ¬
                for table in doc.tables:
                    for row in table.rows:
                        row_text = []
                        for cell in row.cells:
                            if cell.text.strip():
                                row_text.append(cell.text.strip())
                        if row_text:
                            text_content.append(" | ".join(row_text))
                
                full_text = "\n".join(text_content)
            
            # ä½¿ç”¨mammothå¤„ç†.docæ–‡ä»¶
            elif file_path.endswith('.doc'):
                try:
                    with open(file_path, "rb") as docx_file:
                        result = mammoth.convert_to_html(docx_file)
                        # æå–çº¯æ–‡æœ¬ï¼Œå»é™¤HTMLæ ‡ç­¾
                        import re
                        full_text = re.sub('<[^<]+?>', '', result.value)
                except Exception as e:
                    logging.error(f"mammothå¤„ç†.docæ–‡ä»¶å¤±è´¥: {e}")
                    return []
            else:
                logging.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                return []
            
            if not full_text.strip():
                logging.warning(f"Wordæ–‡æ¡£æ— æ–‡æœ¬å†…å®¹: {file_path}")
                return []
            
            chunks = self._split_text(full_text, max_length=1000)
            
            documents = []
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    documents.append({
                        'content': chunk.strip(),
                        'type': 'word_text',
                        'source': file_path,
                        'chunk_id': i,
                        'metadata': {
                            'file_type': 'word',
                            'file_path': file_path,
                            'file_md5': self._calculate_file_md5(file_path),
                            'chunk_size': len(chunk)
                        }
                    })
            
            logging.info(f"Wordæ–‡æ¡£å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"Wordæ–‡æ¡£å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_powerpoint_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†PowerPointæ–‡æ¡£ (.ppt, .pptx)"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            logging.info(f"å¤„ç†PowerPointæ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨python-pptxå¤„ç†.pptxæ–‡ä»¶
            if file_path.endswith('.pptx'):
                prs = Presentation(file_path)
                text_content = []
                
                # æå–å¹»ç¯ç‰‡æ–‡æœ¬
                for slide_num, slide in enumerate(prs.slides):
                    slide_text = []
                    
                    # æå–å½¢çŠ¶æ–‡æœ¬
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text.strip():
                            slide_text.append(shape.text.strip())
                    
                    # æå–è¡¨æ ¼æ–‡æœ¬
                    for shape in slide.shapes:
                        if shape.has_table:
                            table = shape.table
                            for row in table.rows:
                                row_text = []
                                for cell in row.cells:
                                    if cell.text.strip():
                                        row_text.append(cell.text.strip())
                                if row_text:
                                    slide_text.append(" | ".join(row_text))
                    
                    if slide_text:
                        text_content.append(f"å¹»ç¯ç‰‡{slide_num + 1}: " + " | ".join(slide_text))
            
            # å¯¹äº.pptæ–‡ä»¶ï¼Œæš‚æ—¶è·³è¿‡ï¼ˆéœ€è¦å…¶ä»–åº“æ”¯æŒï¼‰
            elif file_path.endswith('.ppt'):
                logging.warning(f"æš‚æ—¶è·³è¿‡.pptæ–‡ä»¶: {file_path}")
                return []
            else:
                logging.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                return []
            
            if not text_content:
                logging.warning(f"PowerPointæ–‡æ¡£æ— æ–‡æœ¬å†…å®¹: {file_path}")
                return []
            
            # å°†æ¯å¼ å¹»ç¯ç‰‡ä½œä¸ºä¸€ä¸ªæ–‡æ¡£å—
            documents = []
            for i, slide_text in enumerate(text_content):
                if slide_text.strip():
                    documents.append({
                        'content': slide_text.strip(),
                        'type': 'powerpoint_text',
                        'source': file_path,
                        'slide_id': i + 1,
                        'metadata': {
                            'file_type': 'powerpoint',
                            'file_path': file_path,
                            'file_md5': self._calculate_file_md5(file_path),
                            'slide_number': i + 1,
                            'content_size': len(slide_text)
                        }
                    })
            
            logging.info(f"PowerPointæ–‡æ¡£å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"PowerPointæ–‡æ¡£å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_excel_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†Excelæ–‡æ¡£ (.xls, .xlsx, .xlsm)"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            logging.info(f"å¤„ç†Excelæ–‡æ¡£: {file_path}")
            
            # ä½¿ç”¨openpyxlå¤„ç†.xlsxå’Œ.xlsmæ–‡ä»¶
            if file_path.endswith(('.xlsx', '.xlsm')):
                wb = openpyxl.load_workbook(file_path, data_only=True)
                text_content = []
                
                for sheet_name in wb.sheetnames:
                    sheet = wb[sheet_name]
                    sheet_text = []
                    
                    # æå–å•å…ƒæ ¼æ–‡æœ¬
                    for row in sheet.iter_rows(values_only=True):
                        row_text = []
                        for cell_value in row:
                            if cell_value is not None and str(cell_value).strip():
                                row_text.append(str(cell_value).strip())
                        if row_text:
                            sheet_text.append(" | ".join(row_text))
                    
                    if sheet_text:
                        text_content.append(f"å·¥ä½œè¡¨: {sheet_name}\n" + "\n".join(sheet_text))
            
            # å¯¹äº.xlsæ–‡ä»¶ï¼Œæš‚æ—¶è·³è¿‡ï¼ˆéœ€è¦pandasæ”¯æŒï¼‰
            elif file_path.endswith('.xls'):
                logging.warning(f"æš‚æ—¶è·³è¿‡.xlsæ–‡ä»¶: {file_path}")
                return []
            else:
                logging.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                return []
            
            if not text_content:
                logging.warning(f"Excelæ–‡æ¡£æ— æ–‡æœ¬å†…å®¹: {file_path}")
                return []
            
            # å°†æ¯ä¸ªå·¥ä½œè¡¨ä½œä¸ºä¸€ä¸ªæ–‡æ¡£å—
            documents = []
            for i, sheet_text in enumerate(text_content):
                if sheet_text.strip():
                    documents.append({
                        'content': sheet_text.strip(),
                        'type': 'excel_text',
                        'source': file_path,
                        'sheet_id': i + 1,
                        'metadata': {
                            'file_type': 'excel',
                            'file_path': file_path,
                            'file_md5': self._calculate_file_md5(file_path),
                            'sheet_number': i + 1,
                            'content_size': len(sheet_text)
                        }
                    })
            
            logging.info(f"Excelæ–‡æ¡£å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"Excelæ–‡æ¡£å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_txt_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            text = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        text = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if text is None:
                logging.error(f"æ— æ³•è¯»å–æ–‡æœ¬æ–‡ä»¶: {file_path}")
                return []
            
            chunks = self._split_text(text, max_length=1000)
            
            documents = []
            for i, chunk in enumerate(chunks):
                if chunk.strip():
                    documents.append({
                        'content': chunk.strip(),
                        'type': 'text',
                        'source': file_path,
                        'chunk_id': i,
                        'metadata': {
                            'file_type': 'txt',
                            'file_path': file_path,
                            'file_md5': self._calculate_file_md5(file_path),
                            'chunk_size': len(chunk)
                        }
                    })
            
            logging.info(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_image_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†å›¾åƒæ–‡ä»¶"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            image = Image.open(file_path)
            
            if image.size[0] > 800 or image.size[1] > 800:
                image = self._fix_pil_compatibility(image, (800, 800))
            
            # OCRè¯†åˆ«
            try:
                ocr_result = self.ocr_reader.readtext(np.array(image))
                text_content = " ".join([item[1] for item in ocr_result if item[1].strip()])
            except Exception as e:
                logging.warning(f"OCRè¯†åˆ«å¤±è´¥: {e}")
                text_content = "å›¾åƒOCRè¯†åˆ«å¤±è´¥"
            
            # å›¾è¡¨ç†è§£
            try:
                caption = self.image_captioner(image)[0]['generated_text']
            except Exception as e:
                caption = "å›¾åƒå†…å®¹æè¿°"
                logging.warning(f"å›¾è¡¨ç†è§£å¤±è´¥: {e}")
            
            combined_content = f"OCRè¯†åˆ«æ–‡æœ¬: {text_content}\nå›¾è¡¨ç†è§£: {caption}"
            
            documents = [{
                'content': combined_content,
                'type': 'image',
                'source': file_path,
                'metadata': {
                    'file_type': 'image',
                    'file_path': file_path,
                    'file_md5': self._calculate_file_md5(file_path),
                    'ocr_text': text_content,
                    'caption': caption,
                    'image_size': image.size
                }
            }]
            
            logging.info(f"å›¾åƒæ–‡ä»¶å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logging.error(f"å›¾åƒå¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _process_video_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        try:
            if not self._check_file_integrity(file_path):
                return []
            
            logging.info(f"æå–è§†é¢‘å…³é”®å¸§: {file_path}")
            
            frames = self._extract_key_frames(file_path)
            audio_features = self._analyze_video_audio(file_path)
            
            documents = []
            
            if frames:
                documents.append({
                    'content': f"è§†é¢‘ä¿¡æ¯: {frames[0].shape[1]}x{frames[0].shape[0]}, æå–{len(frames)}ä¸ªå…³é”®å¸§...",
                    'type': 'video_frames',
                    'source': file_path,
                    'metadata': {
                        'file_type': 'video',
                        'file_path': file_path,
                        'file_md5': self._calculate_file_md5(file_path),
                        'frame_count': len(frames),
                        'frame_size': frames[0].shape if frames else None
                    }
                })
            
            if audio_features:
                documents.append({
                    'content': f"éŸ³é¢‘ç‰¹å¾: èŠ‚æ‹={audio_features.get('tempo', 'N/A')}BPM, é¢‘è°±è´¨å¿ƒ={audio_features.get('spectral_centroid', 'N/A')}Hz...",
                    'type': 'video_audio',
                    'source': file_path,
                    'metadata': {
                        'file_type': 'video',
                        'file_path': file_path,
                        'file_md5': self._calculate_file_md5(file_path),
                        'audio_features': audio_features
                    }
                })
            
            logging.info(f"è§†é¢‘å¤„ç†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logging.error(f"è§†é¢‘å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def _extract_key_frames(self, video_path: str, num_frames: int = 10) -> List[np.ndarray]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                logging.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
                return []
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames == 0:
                logging.warning(f"è§†é¢‘æ–‡ä»¶å¸§æ•°ä¸º0: {video_path}")
                cap.release()
                return []
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logging.info(f"è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}fps, {duration:.1f}ç§’")
            
            frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
            frames = []
            
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret and frame is not None:
                    frames.append(frame)
                else:
                    logging.warning(f"æ— æ³•è¯»å–ç¬¬{idx}å¸§")
            
            cap.release()
            
            if not frames:
                logging.warning(f"æœªæˆåŠŸæå–ä»»ä½•å…³é”®å¸§: {video_path}")
            
            return frames
            
        except Exception as e:
            logging.error(f"å…³é”®å¸§æå–å¤±è´¥: {e}")
            return []
    
    def _analyze_video_audio(self, video_path: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘éŸ³é¢‘ç‰¹å¾"""
        try:
            file_size = os.path.getsize(video_path)
            if file_size < 1024:
                logging.warning(f"è§†é¢‘æ–‡ä»¶è¿‡å°ï¼Œè·³è¿‡éŸ³é¢‘åˆ†æ: {video_path}")
                return {}
            
            try:
                y, sr = librosa.load(video_path, sr=None)
            except Exception as e:
                logging.warning(f"librosaåŠ è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•: {e}")
                try:
                    import soundfile as sf
                    y, sr = sf.read(video_path)
                except Exception as e2:
                    logging.warning(f"soundfileä¹Ÿå¤±è´¥ï¼Œè·³è¿‡éŸ³é¢‘åˆ†æ: {e2}")
                    return {}
            
            if len(y) == 0:
                logging.warning(f"éŸ³é¢‘æ•°æ®ä¸ºç©º: {video_path}")
                return {}
            
            try:
                tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
                spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()
                
                return {
                    'tempo': tempo,
                    'spectral_centroid': spectral_centroid,
                    'duration': len(y) / sr,
                    'sample_rate': sr
                }
                
            except Exception as e:
                logging.warning(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
                return {}
            
        except Exception as e:
            logging.warning(f"éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _split_text(self, text: str, max_length: int = 1000) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå›ºå®šé•¿åº¦çš„å—"""
        if len(text) <= max_length:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + max_length
            
            if end < len(text):
                for i in range(end, max(start, end - 100), -1):
                    if text[i] in 'ã€‚.!?':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def process_file(self, file_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ - åŸºäºMD5å»é‡"""
        # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦é‡å¤
        if self._is_duplicate_file(file_path):
            logging.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {file_path}")
            return []
        
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.pdf':
            return self._process_pdf_file(file_path)
        elif file_ext in ['.txt', '.md']:
            return self._process_txt_file(file_path)
        elif file_ext in ['.doc', '.docx']:
            return self._process_word_file(file_path)
        elif file_ext in ['.ppt', '.pptx']:
            return self._process_powerpoint_file(file_path)
        elif file_ext in ['.xls', '.xlsx', '.xlsm']:
            return self._process_excel_file(file_path)
        elif file_ext in ['.png', '.jpg', '.jpeg', '.bmp', '.gif']:
            return self._process_image_file(file_path)
        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            return self._process_video_file(file_path)
        elif file_ext in ['.mp3', '.wav', '.flac']:
            return self._process_video_file(file_path)
        else:
            logging.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return []
    
    def process_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        if not os.path.exists(directory_path):
            logging.error(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
            return []
        
        all_documents = []
        supported_extensions = {'.pdf', '.txt', '.md', '.doc', '.docx', '.ppt', '.pptx', '.xls', '.xlsx', '.xlsm', '.png', '.jpg', '.jpeg', '.bmp', '.gif', '.mp4', '.avi', '.mov', '.mkv', '.mp3', '.wav', '.flac'}
        
        for file_path in Path(directory_path).rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                logging.info(f"å¤„ç†æ–‡ä»¶: {file_path} (ç±»å‹: {file_path.suffix})")
                documents = self.process_file(str(file_path))
                all_documents.extend(documents)
        
        logging.info(f"ç›®å½•å¤„ç†å®Œæˆ: {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents
    
    def build_md5_database(self, documents: List[Dict[str, Any]], save_path: str = "multimodal_vector_db_md5"):
        """æ„å»ºåŸºäºMD5çš„å‘é‡æ•°æ®åº“"""
        if not documents:
            logging.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦å¤„ç†")
            return
        
        logging.info(f"å¼€å§‹æ„å»ºMD5å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [doc['content'] for doc in documents]
        
        # ç”Ÿæˆå‘é‡
        embeddings = self.text_model.encode(texts, show_progress_bar=True, batch_size=32)
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        index.add(embeddings.astype('float32'))
        
        # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
        os.makedirs(save_path, exist_ok=True)
        
        faiss.write_index(index, os.path.join(save_path, "faiss.index"))
        
        with open(os.path.join(save_path, "metadata.pkl"), 'wb') as f:
            pickle.dump([doc['metadata'] for doc in documents], f)
        
        with open(os.path.join(save_path, "documents.pkl"), 'wb') as f:
            pickle.dump(documents, f)
        
        # ä¿å­˜MD5ç´¢å¼•
        with open(os.path.join(save_path, "file_md5_index.json"), 'w', encoding='utf-8') as f:
            json.dump(self.file_md5_cache, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®
        config = {
            'model_name': 'BAAI/bge-large-zh-v1.5',
            'embedding_dimension': dimension,
            'document_count': len(documents),
            'unique_files': len(self.processed_files),
            'document_types': list(set(doc['type'] for doc in documents)),
            'file_types_processed': list(set(doc['metadata'].get('file_type', 'unknown') for doc in documents)),
            'md5_duplicates_skipped': len(self.file_md5_cache) - len(self.processed_files)
        }
        
        with open(os.path.join(save_path, "config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        logging.info(f"âœ… MD5å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼ä¿å­˜è·¯å¾„: {save_path}")
        logging.info(f"ç´¢å¼•ä¿¡æ¯: {dimension}ç»´, {len(documents)}ä¸ªæ–‡æ¡£")
        logging.info(f"å¤„ç†æ–‡ä»¶æ•°: {len(self.processed_files)}")
        logging.info(f"è·³è¿‡é‡å¤æ–‡ä»¶: {len(self.file_md5_cache) - len(self.processed_files)}")
        
        # ä¿å­˜ç´¢å¼•åˆ°å®ä¾‹å˜é‡
        self.index = index
        self.documents = documents
        self.metadata = [doc['metadata'] for doc in documents]
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict[str, Any], float]]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£"""
        if not hasattr(self, 'index') or not hasattr(self, 'documents'):
            logging.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []
        
        query_embedding = self.text_model.encode([query])
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append((self.documents[idx], float(score)))
        
        return results
    
    def show_database_info(self):
        """æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯"""
        if not hasattr(self, 'index'):
            print("  status: æœªåˆå§‹åŒ–")
            return
        
        print(f"  status: å·²åˆå§‹åŒ–")
        print(f"  document_count: {len(self.documents)}")
        print(f"  index_size: {self.index.ntotal}")
        print(f"  unique_files: {len(self.processed_files)}")
        print(f"  md5_cache_size: {len(self.file_md5_cache)}")

def main():
    """ä¸»å‡½æ•°"""
    print(" ä¼ä¸šçŸ¥è¯†åº“å¤šæ¨¡æ€å‘é‡åŒ–ç³»ç»Ÿ - MD5å»é‡ç‰ˆæœ¬")
    print("=" * 60)
    
    # åˆå§‹åŒ–MD5å‘é‡åŒ–å™¨
    vectorizer = MD5MultimodalVectorizer()
    
    # çœŸå®ä¸šåŠ¡æ–‡æ¡£è·¯å¾„
    business_docs = "/root/autodl-tmp/enterprise_kb/sample_docs"
    video_docs = "/root/autodl-tmp/enterprise_kb/video_docs/äº§å“æ¼”ç¤º"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(business_docs):
        print(f"âŒ ä¸šåŠ¡æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {business_docs}")
        return
    
    if not os.path.exists(video_docs):
        print(f"âŒ è§†é¢‘æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {video_docs}")
        return
    
    print(f"ğŸ“ æ‰«æä¸šåŠ¡æ–‡æ¡£ç›®å½•: {business_docs}")
    print(f" æ‰«æè§†é¢‘æ–‡æ¡£ç›®å½•: {video_docs}")
    
    # å¤„ç†ä¸åŒç±»å‹çš„æ–‡æ¡£
    print("\nğŸ“ å¼€å§‹å¤„ç†æ‰€æœ‰ä¸šåŠ¡æ–‡æ¡£...")
    
    # 1. å¤„ç†ä¸šåŠ¡æ–‡æ¡£ï¼ˆåŒ…æ‹¬æ‰€æœ‰Officeæ–‡æ¡£ï¼‰
    print("ğŸ“„ å¤„ç†ä¸šåŠ¡æ–‡æ¡£...")
    business_docs_list = vectorizer.process_directory(business_docs)
    
    # 2. å¤„ç†è§†é¢‘æ–‡æ¡£
    print("ğŸ¥ å¤„ç†è§†é¢‘æ–‡æ¡£...")
    video_docs_list = vectorizer.process_directory(video_docs)
    
    # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    all_docs = business_docs_list + video_docs_list
    
    if not all_docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
        return
    
    print(f"\nğŸ“Š æ–‡æ¡£å¤„ç†ç»Ÿè®¡:")
    print(f"ä¸šåŠ¡æ–‡æ¡£: {len(business_docs_list)} ä¸ª")
    print(f"è§†é¢‘æ–‡æ¡£: {len(video_docs_list)} ä¸ª")
    print(f"æ€»è®¡: {len(all_docs)} ä¸ªæ–‡æ¡£")
    print(f"å”¯ä¸€æ–‡ä»¶æ•°: {len(vectorizer.processed_files)}")
    
    # æ„å»ºMD5å‘é‡æ•°æ®åº“
    print("\n æ„å»ºMD5å‘é‡æ•°æ®åº“...")
    vectorizer.build_md5_database(all_docs)
    
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    print("\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")
    test_queries = [
        "å•†ä¸šåˆåŒ",
        "è´¢åŠ¡æŠ¥è¡¨", 
        "äº§å“è¯´æ˜",
        "ä¼šè®®è®°å½•",
        "åŸ¹è®­è¯¾ç¨‹",
        "Springlake",
        "Guardia",
        "å¥‘çº¦é”"
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: '{query}'")
        results = vectorizer.search(query, top_k=3)
        if results:
            for i, (doc, score) in enumerate(results, 1):
                print(f"  [{i}] ç›¸ä¼¼åº¦: {score:.3f}")
                print(f"      ç±»å‹: {doc.get('type', 'unknown')}")
                content = doc.get('content', '')[:100]
                print(f"      å†…å®¹: {content}...")
        else:
            print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
    vectorizer.show_database_info()
    
    print("\n MD5å»é‡å‘é‡åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    main()