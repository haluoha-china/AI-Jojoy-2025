#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒæ•°æ®æ–‡ä»¶å‡†å¤‡è„šæœ¬
å°†ç°æœ‰çš„comprehensive_eval.jsonlè½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""

import json
import os
from pathlib import Path

def convert_to_training_format(input_file, train_output, eval_output, train_ratio=0.8):
    """
    å°†comprehensive_eval.jsonlè½¬æ¢ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†
    
    Args:
        input_file: è¾“å…¥çš„comprehensive_eval.jsonlæ–‡ä»¶
        train_output: è®­ç»ƒé›†è¾“å‡ºæ–‡ä»¶
        eval_output: éªŒè¯é›†è¾“å‡ºæ–‡ä»¶
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼Œé»˜è®¤0.8
    """
    
    print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {input_file}")
    
    # è¯»å–æ•°æ®
    data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"æ€»æ•°æ®é‡: {len(data)} æ¡")
    
    # æŒ‰ç¼©ç•¥è¯­åˆ†ç»„ï¼Œç¡®ä¿æ¯ä¸ªç¼©ç•¥è¯­éƒ½æœ‰è®­ç»ƒå’ŒéªŒè¯æ ·æœ¬
    abbr_groups = {}
    for item in data:
        abbr = item.get('abbr', '')
        if abbr not in abbr_groups:
            abbr_groups[abbr] = []
        abbr_groups[abbr].append(item)
    
    print(f"è¦†ç›–ç¼©ç•¥è¯­æ•°é‡: {len(abbr_groups)} ä¸ª")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_data = []
    eval_data = []
    
    for abbr, items in abbr_groups.items():
        # éšæœºæ‰“ä¹±è¯¥ç¼©ç•¥è¯­çš„æ‰€æœ‰æ ·æœ¬
        import random
        random.shuffle(items)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        split_point = int(len(items) * train_ratio)
        
        # æ·»åŠ åˆ°è®­ç»ƒé›†
        train_data.extend(items[:split_point])
        # æ·»åŠ åˆ°éªŒè¯é›†
        eval_data.extend(items[split_point:])
    
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(eval_data)}")
    
    # ä¿å­˜è®­ç»ƒé›†
    with open(train_output, 'w', encoding='utf-8') as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # ä¿å­˜éªŒè¯é›†
    with open(eval_output, 'w', encoding='utf-8') as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_output}")
    print(f"âœ… éªŒè¯é›†å·²ä¿å­˜åˆ°: {eval_output}")
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    print("\næ•°æ®å®Œæ•´æ€§éªŒè¯:")
    print(f"  è®­ç»ƒé›†è¦†ç›–ç¼©ç•¥è¯­: {len(set(item.get('abbr', '') for item in train_data))}")
    print(f"  éªŒè¯é›†è¦†ç›–ç¼©ç•¥è¯­: {len(set(item.get('abbr', '') for item in eval_data))}")
    
    return True

def create_dataset_info():
    """åˆ›å»ºdataset_info.jsoné…ç½®æ–‡ä»¶"""
    
    config = {
        "company_abbreviations_train": {
            "file_name": "train_data_final.jsonl",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        },
        "company_abbreviations_eval": {
            "file_name": "eval_data_final.jsonl",
            "columns": {
                "prompt": "instruction",
                "query": "input",
                "response": "output"
            }
        }
    }
    
    # ç¡®ä¿dataç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    with open('data/dataset_info.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print("âœ… dataset_info.json é…ç½®æ–‡ä»¶å·²åˆ›å»º")

def main():
    """ä¸»å‡½æ•°"""
    print("=== è®­ç»ƒæ•°æ®æ–‡ä»¶å‡†å¤‡è„šæœ¬ ===")
    print("å°†comprehensive_eval.jsonlè½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€æ ¼å¼")
    print()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_file = "comprehensive_eval.jsonl"
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("è¯·ç¡®ä¿comprehensive_eval.jsonlæ–‡ä»¶åœ¨å½“å‰ç›®å½•")
        return
    
    # è¾“å‡ºæ–‡ä»¶å
    train_output = "train_data_final.jsonl"
    eval_output = "eval_data_final.jsonl"
    
    try:
        # è½¬æ¢æ•°æ®æ ¼å¼
        success = convert_to_training_format(input_file, train_output, eval_output)
        
        if success:
            # åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶
            create_dataset_info()
            
            print("\nğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼")
            print("ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒå‘½ä»¤äº†:")
            print()
            print("python src/train.py \\")
            print("  --model_name_or_path /root/autodl-tmp/enterprise_kb/models/transformers/DeepSeek-R1-Distill-Qwen-7B \\")
            print("  --dataset company_abbreviations_train \\")
            print("  --eval_dataset company_abbreviations_eval \\")
            print("  --template qwen \\")
            print("  --finetuning_type lora \\")
            print("  --lora_rank 8 \\")
            print("  --lora_alpha 16 \\")
            print("  --lora_dropout 0.1 \\")
            print("  --lora_target q_proj,v_proj \\")
            print("  --output_dir ./lora_ckpt_prod \\")
            print("  --num_train_epochs 2 \\")
            print("  --per_device_train_batch_size 1 \\")
            print("  --gradient_accumulation_steps 16 \\")
            print("  --learning_rate 5e-5 \\")
            print("  --cutoff_len 256 \\")
            print("  --save_strategy steps \\")
            print("  --save_steps 100 \\")
            print("  --logging_steps 1 \\")
            print("  --overwrite_output_dir \\")
            print("  --bf16 true \\")
            print("  --metric_for_best_model eval_loss \\")
            print("  --load_best_model_at_end true")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
