#!/usr/bin/env python3
"""
ä¼ä¸šçŸ¥è¯†åº“å¤šæ¨¡æ€å‘é‡åŒ–ç³»ç»Ÿ
æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘çš„æ™ºèƒ½å‘é‡åŒ–å’Œæ£€ç´¢
"""

import os
import json
import torch
import cv2
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import logging
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px

# æ–‡æœ¬å¤„ç†
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# éŸ³é¢‘å¤„ç†
import librosa
import soundfile as sf

# å›¾è¡¨ç†è§£
import easyocr
from transformers import pipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MultimodalVectorizer:
    """å¤šæ¨¡æ€å‘é‡åŒ–å™¨ - æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘"""
    
    def __init__(self, 
                 text_model: str = "BAAI/bge-large-zh-v1.5",
                 image_model: str = "microsoft/DialoGPT-medium",
                 device: str = "auto"):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€å‘é‡åŒ–å™¨
        
        Args:
            text_model: æ–‡æœ¬åµŒå…¥æ¨¡å‹
            image_model: å›¾åƒç†è§£æ¨¡å‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device if device != "auto" else ("cuda" if torch.cuda.is_available() else "cpu")
        self.text_model = text_model
        self.image_model = image_model
        
        # åˆå§‹åŒ–å„æ¨¡æ€å¤„ç†å™¨
        self.text_embeddings = None
        self.image_processor = None
        self.ocr_reader = None
        self.chart_analyzer = None
        self.audio_processor = None
        
        # å‘é‡æ•°æ®åº“
        self.vector_db = None
        self.text_splitter = None
        
        logger.info(f"åˆå§‹åŒ–å¤šæ¨¡æ€å‘é‡åŒ–å™¨ï¼Œè®¾å¤‡: {self.device}")
        self._initialize_processors()
    
    def _initialize_processors(self):
        """åˆå§‹åŒ–å„æ¨¡æ€å¤„ç†å™¨"""
        try:
            # 1. æ–‡æœ¬åµŒå…¥æ¨¡å‹
            logger.info("åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
            self.text_embeddings = SentenceTransformer(
                self.text_model,
                device=self.device
            )
            
            # 2. æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
            )
            
            # 3. OCRæ–‡æœ¬è¯†åˆ«
            logger.info("åˆå§‹åŒ–OCRå¤„ç†å™¨...")
            self.ocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=torch.cuda.is_available())
            
            # 4. å›¾è¡¨ç†è§£æ¨¡å‹
            logger.info("åˆå§‹åŒ–å›¾è¡¨ç†è§£æ¨¡å‹...")
            self.chart_analyzer = pipeline(
                "image-to-text",
                model="microsoft/DialoGPT-medium",
                device=0 if self.device == "cuda" else -1
            )
            
            # 5. éŸ³é¢‘å¤„ç†å™¨
            logger.info("åˆå§‹åŒ–éŸ³é¢‘å¤„ç†å™¨...")
            # éŸ³é¢‘å¤„ç†ä½¿ç”¨librosaï¼Œæ— éœ€é¢å¤–æ¨¡å‹
            
            logger.info("âœ… æ‰€æœ‰æ¨¡æ€å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def process_text(self, text: str, metadata: Dict[str, Any] = None) -> Document:
        """å¤„ç†æ–‡æœ¬å†…å®¹"""
        try:
            # åˆ†å‰²é•¿æ–‡æœ¬
            chunks = self.text_splitter.split_text(text)
            
            documents = []
            for i, chunk in enumerate(chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        'type': 'text',
                        'chunk_id': i,
                        'original_length': len(text),
                        **(metadata or {})
                    }
                )
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å¤„ç†å¤±è´¥: {e}")
            return []
    
    def process_image(self, image_path: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†å›¾åƒå†…å®¹ - OCR + å›¾è¡¨ç†è§£"""
        try:
            logger.info(f"å¤„ç†å›¾åƒ: {image_path}")
            
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            
            # è½¬æ¢ä¸ºRGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            documents = []
            
            # 1. OCRæ–‡æœ¬æå–
            logger.info("æ‰§è¡ŒOCRæ–‡æœ¬è¯†åˆ«...")
            ocr_results = self.ocr_reader.readtext(image_path)
            
            if ocr_results:
                # æå–æ‰€æœ‰æ–‡æœ¬
                extracted_text = " ".join([result[1] for result in ocr_results])
                
                # åˆ›å»ºOCRæ–‡æ¡£
                ocr_doc = Document(
                    page_content=f"å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼š{extracted_text}",
                    metadata={
                        'type': 'image_ocr',
                        'source': image_path,
                        'ocr_confidence': np.mean([result[2] for result in ocr_results]),
                        'text_count': len(ocr_results),
                        **(metadata or {})
                    }
                )
                documents.append(ocr_doc)
                
                # å¦‚æœOCRæ–‡æœ¬è¾ƒé•¿ï¼Œè¿›è¡Œåˆ†å‰²
                if len(extracted_text) > 200:
                    text_chunks = self.text_splitter.split_text(extracted_text)
                    for i, chunk in enumerate(text_chunks):
                        chunk_doc = Document(
                            page_content=chunk,
                            metadata={
                                'type': 'image_ocr_chunk',
                                'source': image_path,
                                'chunk_id': i,
                                'ocr_confidence': np.mean([result[2] for result in ocr_results]),
                                **(metadata or {})
                            }
                        )
                        documents.append(chunk_doc)
            
            # 2. å›¾è¡¨ç†è§£ï¼ˆå°è¯•ï¼‰
            try:
                logger.info("å°è¯•å›¾è¡¨ç†è§£...")
                # ä½¿ç”¨PILåŠ è½½å›¾åƒ
                pil_image = Image.open(image_path)
                
                # å›¾è¡¨ç†è§£
                chart_description = self.chart_analyzer(pil_image)[0]['generated_text']
                
                if chart_description and len(chart_description) > 10:
                    chart_doc = Document(
                        page_content=f"å›¾è¡¨åˆ†æç»“æœï¼š{chart_description}",
                        metadata={
                            'type': 'image_chart',
                            'source': image_path,
                            'chart_analysis': True,
                            **(metadata or {})
                        }
                    )
                    documents.append(chart_doc)
                    
            except Exception as chart_e:
                logger.warning(f"å›¾è¡¨ç†è§£å¤±è´¥: {chart_e}")
            
            # 3. å›¾åƒç‰¹å¾æè¿°
            # åˆ†æå›¾åƒåŸºæœ¬ä¿¡æ¯
            height, width = image.shape[:2]
            channels = image.shape[2] if len(image.shape) > 2 else 1
            
            # è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) > 2 else image
            brightness = np.mean(gray)
            contrast = np.std(gray)
            
            image_info_doc = Document(
                page_content=f"å›¾åƒä¿¡æ¯ï¼šå°ºå¯¸{width}x{height}ï¼Œé€šé“æ•°{channels}ï¼Œäº®åº¦{brightness:.1f}ï¼Œå¯¹æ¯”åº¦{contrast:.1f}",
                metadata={
                    'type': 'image_info',
                    'source': image_path,
                    'width': width,
                    'height': height,
                    'channels': channels,
                    'brightness': float(brightness),
                    'contrast': float(contrast),
                    **(metadata or {})
                }
            )
            documents.append(image_info_doc)
            
            logger.info(f"âœ… å›¾åƒå¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
            return []
    
    def process_video(self, video_path: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†è§†é¢‘å†…å®¹ - å…³é”®å¸§æå– + éŸ³é¢‘åˆ†æ"""
        try:
            logger.info(f"å¤„ç†è§†é¢‘: {video_path}")
            
            documents = []
            
            # 1. è§†é¢‘åŸºæœ¬ä¿¡æ¯
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps if fps > 0 else 0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # è§†é¢‘ä¿¡æ¯æ–‡æ¡£
            video_info_doc = Document(
                page_content=f"è§†é¢‘ä¿¡æ¯ï¼šæ—¶é•¿{duration:.1f}ç§’ï¼Œå¸§ç‡{fps:.1f}fpsï¼Œåˆ†è¾¨ç‡{width}x{height}ï¼Œæ€»å¸§æ•°{frame_count}",
                metadata={
                    'type': 'video_info',
                    'source': video_path,
                    'duration': duration,
                    'fps': fps,
                    'frame_count': frame_count,
                    'width': width,
                    'height': height,
                    **(metadata or {})
                }
            )
            documents.append(video_info_doc)
            
            # 2. å…³é”®å¸§æå–å’Œåˆ†æ
            logger.info("æå–å…³é”®å¸§...")
            key_frames = self._extract_key_frames(cap, max_frames=10)
            
            for i, frame in enumerate(key_frames):
                # ä¿å­˜å…³é”®å¸§
                frame_path = f"{video_path}_frame_{i}.jpg"
                cv2.imwrite(frame_path, frame)
                
                # å¤„ç†å…³é”®å¸§
                frame_docs = self.process_image(frame_path, {
                    'type': 'video_keyframe',
                    'video_source': video_path,
                    'frame_index': i,
                    'timestamp': i * (duration / len(key_frames)),
                    **(metadata or {})
                })
                
                documents.extend(frame_docs)
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(frame_path)
            
            # 3. éŸ³é¢‘åˆ†æï¼ˆå¦‚æœè§†é¢‘æœ‰éŸ³é¢‘è½¨é“ï¼‰
            try:
                logger.info("åˆ†æè§†é¢‘éŸ³é¢‘...")
                audio_docs = self._analyze_video_audio(video_path, metadata)
                documents.extend(audio_docs)
            except Exception as audio_e:
                logger.warning(f"è§†é¢‘éŸ³é¢‘åˆ†æå¤±è´¥: {audio_e}")
            
            cap.release()
            
            logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            return []
    
    def _extract_key_frames(self, cap: cv2.VideoCapture, max_frames: int = 10) -> List[np.ndarray]:
        """æå–è§†é¢‘å…³é”®å¸§"""
        frames = []
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if frame_count <= max_frames:
            # å¦‚æœå¸§æ•°ä¸å¤šï¼Œå…¨éƒ¨æå–
            for i in range(frame_count):
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if ret:
                    frames.append(frame)
        else:
            # å‡åŒ€é‡‡æ ·å…³é”®å¸§
            step = frame_count // max_frames
            for i in range(0, frame_count, step):
                if len(frames) >= max_frames:
                    break
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if ret:
                    frames.append(frame)
        
        return frames
    
    def _analyze_video_audio(self, video_path: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """åˆ†æè§†é¢‘éŸ³é¢‘å†…å®¹"""
        try:
            # ä½¿ç”¨librosaåˆ†æéŸ³é¢‘
            y, sr = librosa.load(video_path, sr=None)
            
            # éŸ³é¢‘ç‰¹å¾
            duration = librosa.get_duration(y=y, sr=sr)
            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # åˆ›å»ºéŸ³é¢‘åˆ†ææ–‡æ¡£
            audio_doc = Document(
                page_content=f"éŸ³é¢‘åˆ†æï¼šæ—¶é•¿{duration:.1f}ç§’ï¼ŒèŠ‚æ‹{tempo:.1f}BPMï¼Œé¢‘è°±è´¨å¿ƒ{np.mean(spectral_centroids):.1f}Hz",
                metadata={
                    'type': 'video_audio',
                    'source': video_path,
                    'duration': duration,
                    'tempo': tempo,
                    'sample_rate': sr,
                    'spectral_centroid_mean': float(np.mean(spectral_centroids)),
                    'mfcc_features': mfccs.shape,
                    **(metadata or {})
                }
            )
            
            return [audio_doc]
            
        except Exception as e:
            logger.warning(f"éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            return []
    
    def process_audio(self, audio_path: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†éŸ³é¢‘å†…å®¹"""
        try:
            logger.info(f"å¤„ç†éŸ³é¢‘: {audio_path}")
            
            documents = []
            
            # 1. éŸ³é¢‘åŸºæœ¬ä¿¡æ¯
            y, sr = librosa.load(audio_path, sr=None)
            duration = librosa.get_duration(y=y, sr=sr)
            
            # éŸ³é¢‘ä¿¡æ¯æ–‡æ¡£
            audio_info_doc = Document(
                page_content=f"éŸ³é¢‘ä¿¡æ¯ï¼šæ—¶é•¿{duration:.1f}ç§’ï¼Œé‡‡æ ·ç‡{sr}Hzï¼Œæ–‡ä»¶å¤§å°{os.path.getsize(audio_path)}å­—èŠ‚",
                metadata={
                    'type': 'audio_info',
                    'source': audio_path,
                    'duration': duration,
                    'sample_rate': sr,
                    'file_size': os.path.getsize(audio_path),
                    **(metadata or {})
                }
            )
            documents.append(audio_info_doc)
            
            # 2. éŸ³é¢‘ç‰¹å¾åˆ†æ
            # èŠ‚æ‹æ£€æµ‹
            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            
            # MFCCç‰¹å¾
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # éŸ³é¢‘ç‰¹å¾æ–‡æ¡£
            features_doc = Document(
                page_content=f"éŸ³é¢‘ç‰¹å¾ï¼šèŠ‚æ‹{tempo:.1f}BPMï¼Œé¢‘è°±è´¨å¿ƒ{np.mean(spectral_centroids):.1f}Hzï¼Œé¢‘è°±æ»šé™{np.mean(spectral_rolloff):.1f}Hz",
                metadata={
                    'type': 'audio_features',
                    'source': audio_path,
                    'tempo': tempo,
                    'beat_count': len(beats),
                    'spectral_centroid_mean': float(np.mean(spectral_centroids)),
                    'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),
                    'mfcc_shape': mfccs.shape,
                    **(metadata or {})
                }
            )
            documents.append(features_doc)
            
            # 3. è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆå¦‚æœæœ‰è¯­éŸ³å†…å®¹ï¼‰
            try:
                # è¿™é‡Œå¯ä»¥é›†æˆè¯­éŸ³è¯†åˆ«æ¨¡å‹
                # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºéœ€è¦é¢å¤–çš„è¯­éŸ³è¯†åˆ«æ¨¡å‹
                pass
            except Exception as stt_e:
                logger.debug(f"è¯­éŸ³è½¬æ–‡æœ¬è·³è¿‡: {stt_e}")
            
            logger.info(f"âœ… éŸ³é¢‘å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            return []
    
    def process_file(self, file_path: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©å¤„ç†å™¨"""
        try:
            file_path = str(file_path)
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext in ['.txt', '.md', '.json', '.csv']:
                # æ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return self.process_text(content, metadata)
                
            elif file_ext in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']:
                # å›¾åƒæ–‡ä»¶
                return self.process_image(file_path, metadata)
                
            elif file_ext in ['.mp4', '.avi', '.mov', '.mkv', '.wmv']:
                # è§†é¢‘æ–‡ä»¶
                return self.process_video(file_path, metadata)
                
            elif file_ext in ['.mp3', '.wav', '.flac', '.aac', '.ogg']:
                # éŸ³é¢‘æ–‡ä»¶
                return self.process_audio(file_path, metadata)
                
            elif file_ext in ['.pdf']:
                # PDFæ–‡ä»¶ï¼ˆéœ€è¦pdfminerï¼‰
                try:
                    from pdfminer.high_level import extract_text
                    content = extract_text(file_path)
                    return self.process_text(content, metadata)
                except ImportError:
                    logger.warning("pdfmineræœªå®‰è£…ï¼Œè·³è¿‡PDFå¤„ç†")
                    return []
                
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
                return []
                
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
            return []
    
    def process_directory(self, directory: str, metadata: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†æ•´ä¸ªç›®å½•çš„æ–‡ä»¶"""
        try:
            logger.info(f"å¤„ç†ç›®å½•: {directory}")
            
            if not os.path.exists(directory):
                logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
                return []
            
            all_documents = []
            
            # éå†ç›®å½•
            for root, dirs, files in os.walk(directory):
                for file in files:
                    file_path = os.path.join(root, file)
                    
                    # å¤„ç†æ–‡ä»¶
                    file_docs = self.process_file(file_path, {
                        'directory': directory,
                        'relative_path': os.path.relpath(file_path, directory),
                        **(metadata or {})
                    })
                    
                    all_documents.extend(file_docs)
            
            logger.info(f"âœ… ç›®å½•å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(all_documents)} ä¸ªæ–‡æ¡£")
            return all_documents
            
        except Exception as e:
            logger.error(f"âŒ ç›®å½•å¤„ç†å¤±è´¥: {e}")
            return []
    
    def build_vector_database(self, documents: List[Document], save_path: str = "multimodal_kb_index"):
        """æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“"""
        try:
            logger.info(f"å¼€å§‹æ„å»ºå¤šæ¨¡æ€å‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
            
            if not documents:
                logger.warning("æ²¡æœ‰æ–‡æ¡£éœ€è¦å‘é‡åŒ–")
                return
            
            # ä½¿ç”¨FAISSæ„å»ºå‘é‡æ•°æ®åº“
            self.vector_db = FAISS.from_documents(
                documents=documents,
                embedding=self.text_embeddings
            )
            
            # ä¿å­˜å‘é‡æ•°æ®åº“
            self.vector_db.save_local(save_path)
            
            logger.info(f"âœ… å¤šæ¨¡æ€å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {save_path}")
            logger.info(f"å‘é‡æ•°æ®åº“å¤§å°: {len(self.vector_db.index_to_docstore_id)} ä¸ªå‘é‡")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
            raise
    
    def search_similar(self, query: str, top_k: int = 5, filter_type: str = None) -> List[Dict[str, Any]]:
        """ç›¸ä¼¼æ€§æœç´¢ï¼Œæ”¯æŒç±»å‹è¿‡æ»¤"""
        if not self.vector_db:
            logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []
        
        try:
            # æ‰§è¡Œæœç´¢
            docs_and_scores = self.vector_db.similarity_search_with_score(
                query, 
                k=top_k
            )
            
            results = []
            for doc, score in docs_and_scores:
                # ç±»å‹è¿‡æ»¤
                if filter_type and doc.metadata.get('type') != filter_type:
                    continue
                
                result = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': float(score)
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_database_info(self) -> Dict[str, Any]:
        """è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯"""
        if not self.vector_db:
            return {"status": "æœªåˆå§‹åŒ–"}
        
        try:
            # ç»Ÿè®¡å„ç±»å‹æ–‡æ¡£æ•°é‡
            type_counts = {}
            for doc_id in self.vector_db.index_to_docstore_id.values():
                doc = self.vector_db.docstore._dict[doc_id]
                doc_type = doc.metadata.get('type', 'unknown')
                type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
            
            return {
                "status": "å·²åˆå§‹åŒ–",
                "vector_count": len(self.vector_db.index_to_docstore_id),
                "embedding_dimension": self.text_embeddings.get_sentence_embedding_dimension(),
                "text_model": self.text_model,
                "device": self.device,
                "document_types": type_counts
            }
        except Exception as e:
            return {"status": f"è·å–ä¿¡æ¯å¤±è´¥: {e}"}

def main():
    """ä¸»å‡½æ•° - æ„å»ºå¤šæ¨¡æ€ä¼ä¸šçŸ¥è¯†åº“"""
    print("ğŸš€ å¤šæ¨¡æ€ä¼ä¸šçŸ¥è¯†åº“å‘é‡åŒ–ç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–å¤šæ¨¡æ€å‘é‡åŒ–å™¨
        vectorizer = MultimodalVectorizer()
        
        # 2. å¤„ç†ä¸åŒç±»å‹çš„æ–‡ä»¶
        all_documents = []
        
        # æ–‡æœ¬æ–‡ä»¶
        if os.path.exists("enterprise_terminology_complete.json"):
            print("ğŸ“š å¤„ç†æœ¯è¯­åº“...")
            term_docs = vectorizer.process_file("enterprise_terminology_complete.json", {'category': 'terminology'})
            all_documents.extend(term_docs)
            print(f"  - æœ¯è¯­åº“: {len(term_docs)} ä¸ªæ–‡æ¡£")
        
        # ä¸šåŠ¡æ–‡æ¡£
        if os.path.exists("sample_docs"):
            print("ğŸ“„ å¤„ç†ä¸šåŠ¡æ–‡æ¡£...")
            business_docs = vectorizer.process_directory("sample_docs", {'category': 'business'})
            all_documents.extend(business_docs)
            print(f"  - ä¸šåŠ¡æ–‡æ¡£: {len(business_docs)} ä¸ªæ–‡æ¡£")
        
        # å›¾åƒæ–‡ä»¶
        if os.path.exists("images"):
            print("ğŸ–¼ï¸ å¤„ç†å›¾åƒæ–‡ä»¶...")
            image_docs = vectorizer.process_directory("images", {'category': 'visual'})
            all_documents.extend(image_docs)
            print(f"  - å›¾åƒæ–‡ä»¶: {len(image_docs)} ä¸ªæ–‡æ¡£")
        
        # è§†é¢‘æ–‡ä»¶
        if os.path.exists("videos"):
            print("ğŸ¥ å¤„ç†è§†é¢‘æ–‡ä»¶...")
            video_docs = vectorizer.process_directory("videos", {'category': 'video'})
            all_documents.extend(video_docs)
            print(f"  - è§†é¢‘æ–‡ä»¶: {len(video_docs)} ä¸ªæ–‡æ¡£")
        
        # éŸ³é¢‘æ–‡ä»¶
        if os.path.exists("audios"):
            print("ğŸµ å¤„ç†éŸ³é¢‘æ–‡ä»¶...")
            audio_docs = vectorizer.process_directory("audios", {'category': 'audio'})
            all_documents.extend(audio_docs)
            print(f"  - éŸ³é¢‘æ–‡ä»¶: {len(audio_docs)} ä¸ªæ–‡æ¡£")
        
        print(f"\nğŸ“Š æ€»æ–‡æ¡£æ•°é‡: {len(all_documents)}")
        
        # 3. æ„å»ºå‘é‡æ•°æ®åº“
        if all_documents:
            vectorizer.build_vector_database(all_documents, "multimodal_kb_index")
            
            # 4. æµ‹è¯•æœç´¢åŠŸèƒ½
            print("\nğŸ§ª æµ‹è¯•å¤šæ¨¡æ€æœç´¢åŠŸèƒ½:")
            test_queries = ["AAPæ˜¯ä»€ä¹ˆï¼Ÿ", "å®¢æˆ·ç»ç†èŒè´£", "æ‰“å°æœåŠ¡æµç¨‹", "å›¾è¡¨åˆ†æ", "è§†é¢‘å†…å®¹"]
            
            for query in test_queries:
                print(f"\næŸ¥è¯¢: {query}")
                results = vectorizer.search_similar(query, top_k=3)
                
                for i, result in enumerate(results, 1):
                    print(f"  ç»“æœ {i}: ç›¸ä¼¼åº¦ {result['similarity_score']:.4f}")
                    print(f"    ç±»å‹: {result['metadata'].get('type', 'N/A')}")
                    print(f"    å†…å®¹: {result['content'][:100]}...")
                    print(f"    æ¥æº: {result['metadata'].get('source', 'N/A')}")
            
            # 5. æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
            print("\nğŸ“Š å¤šæ¨¡æ€å‘é‡æ•°æ®åº“ä¿¡æ¯:")
            db_info = vectorizer.get_database_info()
            for key, value in db_info.items():
                print(f"  {key}: {value}")
        
        print("\nğŸ‰ å¤šæ¨¡æ€ä¼ä¸šçŸ¥è¯†åº“å‘é‡åŒ–å®Œæˆï¼")
        print("ä¸‹ä¸€æ­¥ï¼šé›†æˆåˆ°åƒé—®Agentç³»ç»Ÿä¸­ï¼Œæ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢")
        
    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ€å‘é‡åŒ–è¿‡ç¨‹å¤±è´¥: {e}")
        logger.error(f"å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)

if __name__ == "__main__":
    main()
