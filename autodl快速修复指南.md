# ğŸš€ autodlç¯å¢ƒå¿«é€Ÿä¿®å¤æŒ‡å—

## ğŸ” **ä½ çš„autodlç¯å¢ƒç°çŠ¶**

æ ¹æ®ä½ æä¾›çš„ä¿¡æ¯ï¼Œä½ åœ¨autodlä¸Šçš„æƒ…å†µæ˜¯ï¼š

- **é¡¹ç›®è·¯å¾„**: `/root/autodl-tmp/enterprise_kb/LLaMA-Factory`
- **ç°æœ‰æ–‡ä»¶**: `data/comprehensive_eval.jsonl` (270è¡Œæ•°æ®)
- **é—®é¢˜**: ç¼ºå°‘è®­ç»ƒé›†æ–‡ä»¶å’Œæ­£ç¡®çš„é…ç½®

## âœ… **ç«‹å³ä¿®å¤æ­¥éª¤**

### æ­¥éª¤1: ä¸Šä¼ ä¿®å¤è„šæœ¬åˆ°autodl

å°† `fix_autodl_training.py` ä¸Šä¼ åˆ°ä½ çš„autodlç¯å¢ƒï¼Œæˆ–è€…ç›´æ¥åœ¨autodlä¸Šåˆ›å»ºè¿™ä¸ªæ–‡ä»¶ã€‚

### æ­¥éª¤2: åœ¨autodlä¸Šè¿è¡Œä¿®å¤è„šæœ¬

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /root/autodl-tmp/enterprise_kb/LLaMA-Factory

# è¿è¡Œä¿®å¤è„šæœ¬
python fix_autodl_training.py
```

### æ­¥éª¤3: æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶

```bash
# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
ls -la train_data_final.jsonl eval_data_final.jsonl
ls -la data/dataset_info.json
ls -la correct_training_command.sh

# æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
wc -l train_data_final.jsonl eval_data_final.jsonl
```

### æ­¥éª¤4: æ¸…ç†è¾“å‡ºç›®å½•

```bash
# æ¸…ç†ä¹‹å‰çš„è®­ç»ƒè¾“å‡º
rm -rf ./lora_ckpt_prod
```

### æ­¥éª¤5: å¼€å§‹è®­ç»ƒ

```bash
# æ–¹æ³•1: ç›´æ¥è¿è¡Œç”Ÿæˆçš„è„šæœ¬
bash correct_training_command.sh

# æ–¹æ³•2: æ‰‹åŠ¨è¿è¡Œè®­ç»ƒå‘½ä»¤
python src/train.py \
  --model_name_or_path /root/autodl-tmp/enterprise_kb/models/transformers/DeepSeek-R1-Distill-Qwen-7B \
  --dataset company_abbreviations_train \
  --eval_dataset company_abbreviations_eval \
  --template qwen \
  --finetuning_type lora \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target q_proj,v_proj \
  --output_dir ./lora_ckpt_prod \
  --num_train_epochs 2 \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16 \
  --learning_rate 5e-5 \
  --cutoff_len 256 \
  --save_strategy steps \
  --save_steps 100 \
  --logging_steps 1 \
  --overwrite_output_dir \
  --bf16 true \
  --metric_for_best_model eval_loss \
  --load_best_model_at_end true
```

## ğŸ”‘ **å…³é”®ä¿®å¤ç‚¹è¯´æ˜**

### âœ… **æ•°æ®é›†åç§°æ­£ç¡®æ€§**
- **`--dataset company_abbreviations_train`**: å¯¹åº” `dataset_info.json` ä¸­çš„é”®å
- **`--eval_dataset company_abbreviations_eval`**: å¯¹åº” `dataset_info.json` ä¸­çš„é”®å

### âœ… **æ–‡ä»¶æ˜ å°„å…³ç³»**
```json
{
  "company_abbreviations_train": {
    "file_name": "train_data_final.jsonl",  // å®é™…æ–‡ä»¶å
    "columns": {
      "prompt": "instruction",
      "query": "input", 
      "response": "output"
    }
  }
}
```

### âœ… **æ•°æ®åˆ†å‰²ç­–ç•¥**
- å°†ç°æœ‰çš„270æ¡æ•°æ®æŒ‰ç¼©ç•¥è¯­åˆ†ç»„
- æ¯ä¸ªç¼©ç•¥è¯­çš„80%æ ·æœ¬ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºéªŒè¯
- ç¡®ä¿æ¯ä¸ªç¼©ç•¥è¯­éƒ½æœ‰è®­ç»ƒå’ŒéªŒè¯æ ·æœ¬

## ğŸ“Š **é¢„æœŸç»“æœ**

ä¿®å¤å®Œæˆåï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

```
=== autodlç¯å¢ƒæ£€æŸ¥ ===
å½“å‰ç›®å½•: /root/autodl-tmp/enterprise_kb/LLaMA-Factory
âœ… autodlç¯å¢ƒæ£€æŸ¥é€šè¿‡

=== åˆ†æç°æœ‰æ•°æ®æ–‡ä»¶ ===
æ•°æ®æ–‡ä»¶: data/comprehensive_eval.jsonl
æ€»æ•°æ®é‡: 270 æ¡
æ•°æ®å­—æ®µ: ['abbr', 'english', 'chinese', 'instruction', 'input', 'output']
è¦†ç›–ç¼©ç•¥è¯­æ•°é‡: X ä¸ª

=== åˆ›å»ºè®­ç»ƒæ•°æ®æ–‡ä»¶ ===
æŒ‰ç¼©ç•¥è¯­åˆ†ç»„åï¼Œå…± X ä¸ªç¼©ç•¥è¯­
è®­ç»ƒé›†æ ·æœ¬æ•°: XXX
éªŒè¯é›†æ ·æœ¬æ•°: XX

=== åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶ ===
âœ… æ•°æ®é›†é…ç½®æ–‡ä»¶å·²åˆ›å»º: data/dataset_info.json

=== ç”Ÿæˆè®­ç»ƒå‘½ä»¤ ===
âœ… è®­ç»ƒå‘½ä»¤å·²ä¿å­˜åˆ°: correct_training_command.sh
```

## ğŸš¨ **å¦‚æœé‡åˆ°é—®é¢˜**

### é—®é¢˜1: æƒé™ä¸è¶³
```bash
chmod +x correct_training_command.sh
```

### é—®é¢˜2: Pythonç¯å¢ƒé—®é¢˜
```bash
# ç¡®è®¤ç¯å¢ƒ
conda activate kb_enterprise_py310
python --version
```

### é—®é¢˜3: æ–‡ä»¶ä¸å­˜åœ¨
```bash
# æ£€æŸ¥æ–‡ä»¶
ls -la data/
ls -la *.jsonl
```

## ğŸ¯ **éªŒè¯ä¿®å¤æ•ˆæœ**

ä¿®å¤å®Œæˆåï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ï¼š

```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶
cat data/dataset_info.json

# æ£€æŸ¥è®­ç»ƒæ•°æ®
head -3 train_data_final.jsonl
head -3 eval_data_final.jsonl

# å°è¯•å¯åŠ¨è®­ç»ƒï¼ˆæµ‹è¯•é…ç½®ï¼‰
python src/train.py \
  --model_name_or_path /root/autodl-tmp/enterprise_kb/models/transformers/DeepSeek-R1-Distill-Qwen-7B \
  --dataset company_abbreviations_train \
  --eval_dataset company_abbreviations_eval \
  --template qwen \
  --finetuning_type lora \
  --output_dir ./test_ckpt \
  --num_train_epochs 1 \
  --per_device_train_batch_size 1 \
  --max_steps 10 \
  --overwrite_output_dir
```

å¦‚æœæµ‹è¯•è®­ç»ƒèƒ½æ­£å¸¸å¯åŠ¨ï¼Œè¯´æ˜é…ç½®ä¿®å¤æˆåŠŸï¼

## ğŸ“ **éœ€è¦å¸®åŠ©ï¼Ÿ**

å¦‚æœæŒ‰ç…§ä»¥ä¸Šæ­¥éª¤ä»ç„¶æœ‰é—®é¢˜ï¼Œè¯·æä¾›ï¼š
1. `python fix_autodl_training.py` çš„å®Œæ•´è¾“å‡º
2. `ls -la` çš„ç›®å½•åˆ—è¡¨
3. å…·ä½“çš„é”™è¯¯ä¿¡æ¯
